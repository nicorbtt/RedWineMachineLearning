{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, accuracy_score\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read some information about the dataset from its text description file\n",
    "filereader = open(\"WineDataset/winequality.names\", \"r\")\n",
    "for txt_line in filereader:\n",
    "    print(txt_line)\n",
    "filereader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read red wine dataset\n",
    "red_dataset = pd.read_csv('WineDataset/winequality-red.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some information about the red wine dataset imported.\n",
    "# number of entries (1599), coloumns description (missing attribute values?, type value description..), memory usage\n",
    "red_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a short view of red wine dataset\n",
    "red_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for coloumns (count, mean, std, min and max, lower percentile, median, \n",
    "# and upper percentile)\n",
    "red_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the target variable (quality)\n",
    "# From dataset description quality value has a between 0 and 10... let's see how it is distributed.\n",
    "print(np.sort(np.array(red_dataset['quality'].unique())))\n",
    "print(dict(OrderedDict(sorted(dict(Counter(red_dataset['quality'])).items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous steps this time with the white wine dataset\n",
    "# Read white wine dataset\n",
    "white_dataset = pd.read_csv('WineDataset/winequality-white.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some information about the white wine dataset imported.\n",
    "# number of entries (4898), coloumns description (missing attribute values?, type value description..), memory usage\n",
    "white_dataset.info()\n",
    "# --> the columns are equivalent to red wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a short view of white wine dataset\n",
    "white_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for coloumns (count, mean, std, min and max, lower percentile, median, \n",
    "# and upper percentile)\n",
    "white_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the target variable (quality)\n",
    "# From dataset description quality value has a between 0 and 10... let's see how it is distributed.\n",
    "print(np.sort(np.array(white_dataset['quality'].unique())))\n",
    "print(dict(OrderedDict(sorted(dict(Counter(white_dataset['quality'])).items()))))\n",
    "# --> more open range (9 wine quality) than red wine dataset but same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between features and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's study the correlation between features for each dataset and then and then we compare the correlations \n",
    "# between the two datasets.\n",
    "# To evaluate the linear correlation between two variables has been used the Pearson correlation coefficient. \n",
    "# It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and \n",
    "# −1 is total negative linear correlation.\n",
    "\n",
    "headers = list(red_dataset.columns.values)\n",
    "allCorrCoefRed = []\n",
    "allHoverTextRed = []\n",
    "allCorrCoefWhite = []\n",
    "allHoverTextWhite = []\n",
    "\n",
    "for i in range(len(headers)):\n",
    "    corrCoefRed = []\n",
    "    hoverTextRed = []\n",
    "    corrCoefWhite = []\n",
    "    hoverTextWhite = []\n",
    "    for j in range(len(headers)):\n",
    "        if (j>i):\n",
    "            corrCoefRed.append(None)\n",
    "            hoverTextRed.append(None)\n",
    "            corrCoefWhite.append(None)\n",
    "            hoverTextWhite.append(None)\n",
    "            break\n",
    "        red = np.corrcoef(red_dataset[headers[i]].values, red_dataset[headers[j]].values)[1,0]\n",
    "        white = np.corrcoef(white_dataset[headers[i]].values, white_dataset[headers[j]].values)[1,0]\n",
    "        hoverTextRed.append(\"Correlation between '\" + headers[i] + \"' and '\" + headers[j] + \"' is: \" + str(round(red,2)));\n",
    "        corrCoefRed.append(red)\n",
    "        hoverTextWhite.append(\"Correlation between '\" + headers[i] + \"' and '\" + headers[j] + \"' is: \" + str(round(white,2)));\n",
    "        corrCoefWhite.append(white)\n",
    "    allCorrCoefRed.append(corrCoefRed)\n",
    "    allHoverTextRed.append(hoverTextRed)\n",
    "    allCorrCoefWhite.append(corrCoefWhite)\n",
    "    allHoverTextWhite.append(hoverTextWhite)\n",
    "    \n",
    "data = [go.Heatmap(\n",
    "            z = allCorrCoefRed,\n",
    "            x = headers,\n",
    "            y = headers,\n",
    "            hoverinfo = \"text\",\n",
    "            hovertext = allHoverTextRed,\n",
    "            colorscale='IceFire')]\n",
    "layout = go.Layout(title='<b>Red Wine</b> Correlation between all features and quality.')\n",
    "py.iplot(go.Figure(data, layout))\n",
    "\n",
    "data = [go.Heatmap(\n",
    "            z = allCorrCoefWhite,\n",
    "            x = headers,\n",
    "            y = headers,\n",
    "            hoverinfo = \"text\",\n",
    "            hovertext = allHoverTextWhite,\n",
    "            colorscale='IceFire')]\n",
    "layout = go.Layout(title='<b>White Wine</b> Correlation between all features and quality.')\n",
    "py.iplot(go.Figure(data, layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare correlation between the two dataset to to test the possibility of conducting a study that \n",
    "# includes both datasets. \n",
    "\n",
    "qualityCorrCoefRed = dict()\n",
    "for i in range(len(allCorrCoefRed[11])-1):\n",
    "    qualityCorrCoefRed[headers[i]] = allCorrCoefRed[11][i] \n",
    "qualityCorrCoefRed = dict(sorted(qualityCorrCoefRed.items(), key=lambda kv: abs(kv[1]), reverse=True))\n",
    "\n",
    "qualityCorrCoefWhite = dict()\n",
    "for i in range(len(allCorrCoefWhite[11])-1):\n",
    "    qualityCorrCoefWhite[headers[i]] = allCorrCoefWhite[11][i] \n",
    "qualityCorrCoefWhite = dict(sorted(qualityCorrCoefWhite.items(), key=lambda kv: abs(kv[1]), reverse=True))\n",
    "\n",
    "data = [\n",
    "    go.Bar(name='Red', \n",
    "            x=list(qualityCorrCoefRed.values()), \n",
    "            y=list(qualityCorrCoefRed.keys()), \n",
    "            orientation='h',\n",
    "            marker_color='#AC1E44'),\n",
    "    go.Bar(name='White', \n",
    "            x=list(qualityCorrCoefWhite.values()), \n",
    "            y=list(qualityCorrCoefWhite.keys()), \n",
    "            orientation='h',\n",
    "            marker_color='#aee1e5')\n",
    "]\n",
    "layout = go.Layout(title='<b>Red and White Wine</b> Correlation between features and <i>quality</i> (target).', barmode='group')\n",
    "py.iplot(go.Figure(data, layout))\n",
    "\n",
    "# The correlations will result to be similar in some cases but totally opposite in others with the conclusion\n",
    "# that it is better to conduct two separate studies for white and red wines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the red wine dataset for this study\n",
    "# Let's produce an overiew on each feature and target (quality) correlation to have a visual representation of the\n",
    "# correlation and the presence of outliers.\n",
    "\n",
    "features = list(red_dataset)\n",
    "target = features[-1]\n",
    "features = features[:-1]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows = 4, \n",
    "    cols = 3,\n",
    "    horizontal_spacing = 0.08,\n",
    "    vertical_spacing = 0.03,\n",
    ")\n",
    "\n",
    "r = c = 1\n",
    "for feature in features:\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y = red_dataset[feature], \n",
    "            x=red_dataset[target],\n",
    "            boxpoints='outliers',\n",
    "            jitter=0.3,\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                opacity=0.5,\n",
    "                color='rgb(8,81,156)',\n",
    "                outliercolor='rgb(219, 64, 82)',\n",
    "            ),\n",
    "            line_color='rgb(8,81,156)',\n",
    "            line_width=1,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row = r, col = c)\n",
    "    fig.update_yaxes(title_text=feature, ticks=\"inside\", nticks=7, row=r, col=c)\n",
    "    fig.update_xaxes(ticks=\"inside\", tick0=1, dtick=1, row=r, col=c)\n",
    "    if (r == 4):\n",
    "        fig.update_xaxes(title_text=target, row=r, col=c)\n",
    "    if (r == 3 & c == 3):\n",
    "        fig.update_xaxes(title_text=target, row=r, col=c)\n",
    "    c = c + 1\n",
    "    if (c > 3):\n",
    "        c = 1\n",
    "        r = r + 1\n",
    "        \n",
    "fig.update_layout(title_text=\"BoxPlot overview of features and target correlation with outliers check.\", \n",
    "                  height=1000, \n",
    "                  width=1000)\n",
    "fig.show()\n",
    "\n",
    "# Boxplots show many outliers for quite a few columns. The 'red_dataset.describe()' table output produced some \n",
    "# cells above explain analytically this fact.\n",
    "# Some features as residual sugar, chlorides and sulphates present an huge gap between min and max value. This\n",
    "# could explain the outliers. \n",
    "# In some features a linear correlation is observed more than in others (alcohol, volatile acidity, sulphates...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions...\n",
    "def dataEstimateAndRegressionLine(w0, w1, feature, featureName, target, targetName):\n",
    "    x_line = np.array([np.min(feature), np.max(feature)])\n",
    "    y_line = w0 + w1*x_line\n",
    "    data = [\n",
    "        go.Scatter(\n",
    "            x=feature, \n",
    "            y=target, \n",
    "            mode=\"markers\", \n",
    "            marker_color = \"#000000\",\n",
    "            marker_opacity = 0.7,\n",
    "            marker_line_width=1, \n",
    "            marker_size=8,\n",
    "            marker_line_color = \"#ffffff\",\n",
    "            name=\"data\"\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=feature, \n",
    "            y=y_hat, \n",
    "            mode=\"markers\", \n",
    "            marker_color = \"#007fff\",\n",
    "            marker_opacity = 0.7,\n",
    "            marker_line_width=1, \n",
    "            marker_size=8,\n",
    "            marker_line_color = \"#ffffff\",\n",
    "            name=\"estimate\"\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=x_line, \n",
    "            y=y_line, \n",
    "            mode=\"lines\", \n",
    "            name=\"regression line\"\n",
    "        )\n",
    "    ]\n",
    "    for i in range(len(feature)):\n",
    "        data.append(\n",
    "            go.Scatter(\n",
    "                x=[feature[i], feature[i]], \n",
    "                y=[target[i], y_hat[i]], \n",
    "                mode=\"lines\",\n",
    "                showlegend=False, \n",
    "                line=dict(color=\"gray\", width=0.5)\n",
    "            )\n",
    "        )\n",
    "    layout = go.Layout(\n",
    "        xaxis = dict(title=featureName),\n",
    "        yaxis = dict(title=targetName)\n",
    "    )\n",
    "    return data, layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualPlotAndHistogram(feature, featureName, target, y_hat):\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = feature, \n",
    "            y = target - y_hat, \n",
    "            mode = \"markers\",\n",
    "            marker_color = \"#007fff\",\n",
    "            marker_opacity = 0.7,\n",
    "            marker_line_width = 1, \n",
    "            marker_size = 8,\n",
    "            marker_line_color = \"#ffffff\",\n",
    "            showlegend = False\n",
    "        ),\n",
    "        row = 1, col = 1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            y=target - y_hat, \n",
    "            showlegend = False,\n",
    "            marker = dict(\n",
    "                color = \"#007fff\",\n",
    "            ),\n",
    "        ),\n",
    "        row = 1, col = 2\n",
    "    )\n",
    "    fig.update_xaxes(title_text = featureName, row = 1, col = 1)\n",
    "    fig.update_yaxes(title_text = \"{} Residual\".format(featureName), row = 1, col = 1)\n",
    "    fig.update_xaxes(title_text = \"Number of instances\", row = 1, col = 2)\n",
    "    fig.update_layout(\n",
    "        title = 'Plot and histogram of the residual'\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerformance(y, y_hat):       \n",
    "    err = (y-y_hat)**2 \n",
    "    SSE = np.sum(err)\n",
    "    errMean = (y-np.mean(y))**2\n",
    "    R2 = 1 - (SSE/( np.sum(errMean))) \n",
    "    return SSE, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjustedRSquare(R2, n, p):\n",
    "    # n is number of observations in sample\n",
    "    # p is number of independent variables in model\n",
    "    adjr2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "    return adjr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodnessIndicators(SSE, R2, title, text1, text2):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Indicator(\n",
    "        title = text1,\n",
    "        mode = 'number',\n",
    "        value = SSE,\n",
    "        domain = {'row': 0, 'column': 0}))\n",
    "    fig.add_trace(go.Indicator(\n",
    "        title = text2,\n",
    "        mode = 'number',\n",
    "        value = R2,\n",
    "        domain = {'row': 0, 'column': 1}))\n",
    "    fig.update_layout(\n",
    "        height = 300,\n",
    "        title = title,\n",
    "        grid = {'rows': 1, 'columns': 2, 'pattern': \"independent\"}\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyScoreIndicator(score):\n",
    "    fig = go.Figure(\n",
    "        go.Indicator(\n",
    "            title = 'Accuracy Score',\n",
    "            mode = 'number',\n",
    "            value = score,\n",
    "            )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        height = 300\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(x):\n",
    "    print(type(x))\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate linear regression (alcohol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's start modelling with a simple univariate linear regression using alcohol feature and quality target.\n",
    "feature = red_dataset[\"alcohol\"].values\n",
    "target = red_dataset[\"quality\"].values\n",
    "feature_reshape = np.reshape(feature,(len(feature),1))\n",
    "# Build the model\n",
    "linReg = LinearRegression()\n",
    "linReg.fit(feature_reshape, target)\n",
    "w0 = linReg.intercept_\n",
    "w1 = linReg.coef_\n",
    "# Predict\n",
    "y_hat = linReg.predict(feature_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphically represent the data and the regression line\n",
    "data, layout = dataEstimateAndRegressionLine(w0, w1, feature, \"Alcohol\", target, \"Quality\")\n",
    "py.iplot(go.Figure(data, layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphically represent the residual\n",
    "fig = residualPlotAndHistogram(feature, \"Alcohol\", target, y_hat)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the score of the model\n",
    "# SSE stands for Sum of Squared Errors\n",
    "# R square is the percentage of the response variable variation that is explained by a linear model [0-1]\n",
    "SSE, R2 = computePerformance(target,y_hat)\n",
    "mse = mean_squared_error(target,y_hat)\n",
    "fig = goodnessIndicators(mse, R2, \"Assessing goodness...\", \"MSE\", \"R\\u00b2\")\n",
    "fig.show()\n",
    "\n",
    "# ... high SSE and low model explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate linear regression (volatile acidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another simple univariate linear regression using volatile acidity feature and quality target.\n",
    "\n",
    "feature = red_dataset[\"volatile acidity\"].values\n",
    "feature_reshape = np.reshape(feature,(len(feature),1))\n",
    "# Build the model\n",
    "linReg = LinearRegression()\n",
    "linReg.fit(feature_reshape, target)\n",
    "# Predict\n",
    "y_hat = linReg.predict(feature_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the score of the model\n",
    "SSE, R2 = computePerformance(target,y_hat)\n",
    "mse = mean_squared_error(target,y_hat)\n",
    "fig = goodnessIndicators(mse, R2, \"Assessing goodness...\", \"MSE\", \"R\\u00b2\")\n",
    "fig.show()\n",
    "\n",
    "# ... higher SSE then previous model and even low model explanation (as expected by correlation study done previously)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Multivariate linear regression (alcohol + volatile acidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modelling now with a simple multivariate linear regression using alcohol and volatile acidity feature \n",
    "# and quality target.\n",
    "\n",
    "features = red_dataset[['alcohol', 'volatile acidity']].values\n",
    "target = red_dataset['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(features, target, test_size=0.20, random_state=0)\n",
    "# Build the model\n",
    "linReg = LinearRegression()\n",
    "linReg.fit(xTrain, yTrain)\n",
    "w = linReg.coef_\n",
    "w0 = linReg.intercept_\n",
    "# Take a look at w values\n",
    "print(w)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphically represent the data and the 3D plane\n",
    "x_line = [np.min(xTrain[:,0]), np.max(xTrain[:,0])]\n",
    "y_line = [np.min(xTrain[:,1]), np.max(xTrain[:,1])]\n",
    "z_line = np.zeros((2,2))\n",
    "\n",
    "for ind1 in range(2):\n",
    "    for ind2 in range(2):\n",
    "        xpred = np.array([ [x_line[ind1], y_line[ind2] ] ]) \n",
    "        z_line[ind1,ind2] = linReg.predict(xpred)\n",
    "\n",
    "\n",
    "trace = [go.Scatter3d(x = xTrain[:,0], y = xTrain[:,1], z = target, mode = 'markers', marker = dict(size = 3), name = 'measured data' ),\n",
    "         go.Surface(x = x_line, y = y_line, z = z_line, name = 'linear regression')]\n",
    "layout = go.Layout(scene = dict(xaxis = dict(title = 'Alcohol'), \n",
    "                                yaxis = dict(title = 'Volatile Acidity'), \n",
    "                                zaxis = dict(title = 'Quality')),\n",
    "                  title = '3D Scatter plot with regression plan (Training Set). <b>Alcohol</b> + <b>Volatile Acidity</b> features, <b>Quality</b> target')\n",
    "\n",
    "fig = go.Figure(trace, layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhatTrain = linReg.predict(xTrain)\n",
    "yhatTest = linReg.predict(xTest)\n",
    "# Evaluate the model for training set\n",
    "R2Train = r2_score(yTrain, yhatTrain)\n",
    "adjR2Train = AdjustedRSquare(R2Train, len(xTrain), len(xTrain[0]))\n",
    "mseTrain = mean_squared_error(yTrain, yhatTrain)\n",
    "# Evaluate the model for test set\n",
    "R2Test = r2_score(yTest, yhatTest)\n",
    "adjR2Test = AdjustedRSquare(R2Test, len(xTest), len(xTest[0]))\n",
    "mseTest = mean_squared_error(yTest, yhatTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the score of the model\n",
    "fig = goodnessIndicators(\n",
    "    mseTrain, \n",
    "    adjR2Train, \n",
    "    \"Assessing goodness in Training set:\", \"MSE\", \"Adjusted R\\u00b2\")\n",
    "fig.show()\n",
    "fig = goodnessIndicators(\n",
    "    mseTest, \n",
    "    adjR2Test, \n",
    "    \"Assessing goodness in Test set:\", \"MSE\", \"Adjusted R\\u00b2\")\n",
    "fig.show()\n",
    "\n",
    "# As expected, R squared increase adding more features to the model, even if they are unrelated to the response. \n",
    "# Selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "# In this 'goodness assessing' I have used the Adjusted R squared, more suitable for multivariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression with StratifiedKFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For a better approach to feature selection cross-validation provides a more reliable estimate of out-of-sample \n",
    "# error, and thus is a better way to choose which of your models will best generalize to out-of-sample data.\n",
    "\n",
    "# Stratified K-Folds cross-validator provides train/test indices to split data in train/test sets. This \n",
    "# cross-validation object is a variation of KFold that returns stratified folds. The folds are made by \n",
    "# preserving the percentage of samples for each class.\n",
    "# --> This is relevant to our case in which the target variable is not uniformly distributed\n",
    "\n",
    "featuresList = list(red_dataset)\n",
    "targett = featuresList[-1]\n",
    "featuresList = featuresList[:-1]\n",
    "features = red_dataset[featuresList].values\n",
    "target = red_dataset[targett].values\n",
    "\n",
    "# Generate indices fro cross-validation\n",
    "skf = StratifiedKFold(n_splits = 10) #The least populated class in target (wine with quality=3) has only 10 members. \n",
    "skf.get_n_splits(features)\n",
    "\n",
    "# In this study I used MSE, which stands for mean squared errors and Adjusted R square for evaluate the regression \n",
    "# model\n",
    "\n",
    "MSETrainAll = [] \n",
    "MSETestAll = []\n",
    "AdjustedR2TrainAll = []\n",
    "AdjustedR2TestAll = []\n",
    "\n",
    "for train_index, test_index in skf.split(features, target):\n",
    "    xTrain, xTest = features[train_index], features[test_index]\n",
    "    yTrain, yTest = target[train_index], target[test_index]\n",
    "    # Build the regressor\n",
    "    model = LinearRegression()\n",
    "    # Fit\n",
    "    model.fit(xTrain, yTrain)\n",
    "    # Predict\n",
    "    yhatTrain = model.predict(xTrain)\n",
    "    yhatTest = model.predict(xTest)\n",
    "    # Evaluate the model for training set\n",
    "    R2Train = r2_score(yTrain, yhatTrain)\n",
    "    adjR2Train = AdjustedRSquare(R2Train, len(xTrain), len(xTrain[0]))\n",
    "    mseTrain = mean_squared_error(yTrain, yhatTrain)\n",
    "    # Evaluate the model for test set\n",
    "    R2Test = r2_score(yTest, yhatTest)\n",
    "    adjR2Test = AdjustedRSquare(R2Test, len(xTest), len(xTest[0]))\n",
    "    mseTest = mean_squared_error(yTest, yhatTest)\n",
    "    # Collect values for final mean assessing\n",
    "    MSETrainAll.append(mseTrain)\n",
    "    MSETestAll.append(mseTest)\n",
    "    AdjustedR2TrainAll.append(adjR2Train)\n",
    "    AdjustedR2TestAll.append(adjR2Test)\n",
    "# Present the result by calculating the average of single cross-validation result\n",
    "fig = goodnessIndicators(\n",
    "    np.mean(MSETrainAll), \n",
    "    np.mean(AdjustedR2TrainAll), \n",
    "    \"Assessing goodness in Training set:\", \"MSE\", \"Adjusted R\\u00b2\")\n",
    "fig.show()\n",
    "fig = goodnessIndicators(\n",
    "    np.mean(MSETestAll), \n",
    "    np.mean(AdjustedR2TestAll), \n",
    "    \"Assessing goodness in Test set:\", \"MSE\", \"Adjusted R\\u00b2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression with Ridge and Lasso Regolarization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work now with Ridge and Lasso regolarization maintaining Stratified K-Folds cross-validator as the \n",
    "# previous study to be able to evaluate a possible improvement.\n",
    "# The regolarizations have been built with different alpha values taken from this set:\n",
    "# 1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20, 1e2, 1e3\n",
    "# Then the alhpa for each regolarizations that takes to the best score is used for the evaluation of the regressor\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10) #The least populated class in target has only 10 members.\n",
    "skf.get_n_splits(features)\n",
    "\n",
    "wLassoAll = []\n",
    "wRidgeAll = []\n",
    "LassoMSETestAll = []\n",
    "LassoAdjustedR2TestAll = []\n",
    "RidgeMSETestAll = []\n",
    "RidgeAdjustedR2TestAll = []\n",
    "alphaValues = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20, 1e2, 1e3]\n",
    "for alpha in alphaValues:\n",
    "\n",
    "    wLasso = []\n",
    "    wRidge = []\n",
    "    scoreLasso = []\n",
    "    scoreRidge = []\n",
    "    mseLasso = []\n",
    "    mseRidge = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(features, target):\n",
    "        xTrain, xTest = features[train_index], features[test_index]\n",
    "        yTrain, yTest = target[train_index], target[test_index]\n",
    "\n",
    "        model = Lasso(alpha=alpha, max_iter=10e5)\n",
    "        model.fit(xTrain, yTrain)\n",
    "        w = model.coef_\n",
    "        wLasso.append(w)\n",
    "        yhatTest = model.predict(xTest)\n",
    "        R2Test = r2_score(yTest, yhatTest)\n",
    "        adjR2Test = AdjustedRSquare(R2Test, len(xTest), len(xTest[0]))\n",
    "        mseTest = mean_squared_error(yTest, yhatTest)\n",
    "        scoreLasso.append(adjR2Test)\n",
    "        mseLasso.append(mseTest)\n",
    "        \n",
    "        model2 = Ridge(alpha=alpha, max_iter=10e5)\n",
    "        model2.fit(xTrain, yTrain)\n",
    "        w = model2.coef_\n",
    "        wRidge.append(w)\n",
    "        yhatTest = model2.predict(xTest)\n",
    "        R2Test = r2_score(yTest, yhatTest)\n",
    "        adjR2Test = AdjustedRSquare(R2Test, len(xTest), len(xTest[0]))\n",
    "        mseTest = mean_squared_error(yTest, yhatTest)\n",
    "        scoreRidge.append(adjR2Test)\n",
    "        mseRidge.append(mseTest)\n",
    "        \n",
    "    \n",
    "    LassoMSETestAll.append(np.mean(mseLasso))\n",
    "    LassoAdjustedR2TestAll.append(np.mean(scoreLasso))\n",
    "    RidgeMSETestAll.append(np.mean(mseRidge))\n",
    "    RidgeAdjustedR2TestAll.append(np.mean(scoreRidge))\n",
    "    \n",
    "    wLassoAll.append(np.mean(wLasso, axis=0))\n",
    "    wRidgeAll.append(np.mean(wRidge, axis=0))\n",
    "    \n",
    "lassoAlphaI = 0\n",
    "maxScoreLasso = LassoAdjustedR2TestAll[0]\n",
    "ridgeAlphaI = 0\n",
    "maxScoreRidge = RidgeAdjustedR2TestAll[0]\n",
    "i = 0\n",
    "for score in LassoAdjustedR2TestAll:\n",
    "    if (score > maxScoreLasso):\n",
    "        lassoAlphaI = i\n",
    "        maxScoreLasso = score\n",
    "    i = i + 1\n",
    "i = 0\n",
    "for score in RidgeAdjustedR2TestAll:\n",
    "    if (score > maxScoreRidge):\n",
    "        ridgeAlphaI = i\n",
    "        maxScoreRidge = score\n",
    "    i = i + 1\n",
    "# Print best alpha for both regolarizations\n",
    "print(\"Best alpha for Ridge: \" + str(alphaValues[ridgeAlphaI]))\n",
    "print(\"Best alpha for Lasso: \" + str(alphaValues[lassoAlphaI]))\n",
    "# Take a look at the score of the model\n",
    "fig = goodnessIndicators(\n",
    "    LassoMSETestAll[lassoAlphaI],\n",
    "    LassoAdjustedR2TestAll[lassoAlphaI],\n",
    "    \"<b>Ridge regolarization</b> assessing goodness in Test set:\", \"MSE\", \"Adjusted R\\u00b2\")\n",
    "fig.show()\n",
    "fig = goodnessIndicators(\n",
    "    RidgeMSETestAll[ridgeAlphaI],\n",
    "    RidgeAdjustedR2TestAll[ridgeAlphaI],\n",
    "    \"<b>Lasso regolarization</b> assessing goodness in Test set:\", \"MSE\", \"Adjusted R\\u00b2\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# --> a slight improvement was made to the score but nothing considerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphically represent the W coefficients for each regolarizations for each alpha value\n",
    "def getW(i, coll):\n",
    "    w = []\n",
    "    for array in coll:\n",
    "        w.append(array[i])\n",
    "    return w\n",
    "\n",
    "alphaValuesStr = []\n",
    "for n in alphaValues:\n",
    "    alphaValuesStr.append(str(n))\n",
    "\n",
    "featuresList = list(red_dataset)\n",
    "featuresList = featuresList[:-1]\n",
    "wAll = [wRidgeAll, wLassoAll]\n",
    "\n",
    "density = 0\n",
    "i=0\n",
    "for f in featuresList:\n",
    "    if (f == \"density\"):\n",
    "        density = i\n",
    "    i = i + 1\n",
    "\n",
    "titles = [\"<b>Ridge regolarization</b> coefficients at the increase of alpha\", \n",
    "          \"<b>Lasso regolarization</b> coefficients at the increase of alpha\"]\n",
    "titI = 0\n",
    "for ws in wAll:\n",
    "    fig = go.Figure()\n",
    "    for i in range(len(ws[0])):\n",
    "        # Skip density w because of its huge value that ruin the graphics\n",
    "        if (i == density): continue\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = list(range(len(alphaValues))), \n",
    "                y = getW(i, ws), \n",
    "                line_shape='spline',\n",
    "                name = featuresList[i]\n",
    "            )\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title = titles[titI],\n",
    "        height = 400\n",
    "    ),\n",
    "    titI = titI + 1\n",
    "    fig.update_yaxes(\n",
    "        title = \"w\",\n",
    "    ),\n",
    "    fig.update_xaxes(\n",
    "        title = \"Alpha\",\n",
    "        ticktext=alphaValuesStr,\n",
    "        tickvals=list(range(len(ws[0])+1)),\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "# --> Remember: best alpha for Ridge regression was 1 and for Lasso 0.0001\n",
    "# Ridge regolarization shows how the coefficient tend to zero at the same alpha values while Lasso takes all \n",
    "# coefficient to zero in an uncoordinated way (some are zero before others) that could be useul to perform \n",
    "# a feature selection analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the problem into a classification one. We want now to classificate each istance of red wine, based\n",
    "# on features, into three categories: Bad wine, Average wine and Good wine\n",
    "BadAverageOrGood = []\n",
    "for v in target:\n",
    "    if (v <= 4):\n",
    "        BadAverageOrGood.append(1)\n",
    "        continue\n",
    "    if (v <= 6):\n",
    "        BadAverageOrGood.append(2)\n",
    "        continue\n",
    "    BadAverageOrGood.append(3)\n",
    "counterForEachClass = dict(OrderedDict(sorted(dict(Counter(BadAverageOrGood)).items())))\n",
    "print(counterForEachClass)\n",
    "BadAverageOrGood = np.array(BadAverageOrGood)\n",
    "percentForEachClass = dict()\n",
    "for i in range(len(np.unique(BadAverageOrGood))):\n",
    "    percentForEachClass[i+1] = counterForEachClass[i+1] / len(BadAverageOrGood)\n",
    "print(percentForEachClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = red_dataset[list(red_dataset)[:-1]].values\n",
    "\n",
    "# Before making any actual predictions, it is always a good practice to scale the features so that all of them \n",
    "# can be uniformly evaluated. Wikipedia explains the reasoning pretty well:\n",
    "#      \"Since the range of values of raw data varies widely, in some machine learning algorithms, objective \n",
    "#      functions will not work properly without normalization. For example, the majority of classifiers calculate \n",
    "#      the distance between two points by the Euclidean distance. If one of the features has a broad range of \n",
    "#      values, the distance will be governed by this particular feature. Therefore, the range of all features \n",
    "#      should be normalized so that each feature contributes approximately proportionately to the final distance.\"\n",
    "sc = StandardScaler()\n",
    "features = sc.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start modelling. All model are evalueted using train and test set achieved slpitting the dataset in two parts,\n",
    "# one for training set (80%) and the other for test set (20%). the stratified option in sklearn train_test_split\n",
    "# keeps equal proportions of each class train/test set.\n",
    "# Evaluation concerns accuracy and misclassification costs.\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(\n",
    "    features, \n",
    "    BadAverageOrGood, \n",
    "    test_size=0.20, \n",
    "    stratify=BadAverageOrGood, \n",
    "    random_state=0)\n",
    "misclassificationMatrix = [[0, 1, 5], [1, 0, 1], [5, 1, 0]]\n",
    "\n",
    "# Some functions:\n",
    "def evaluateClassificator(cm, out):\n",
    "    df = pd.DataFrame(cm,\n",
    "             index = [['','MEASURED',''], ['Bad', 'Average', 'Good']],\n",
    "             columns = [['','PREDICTED',''],['Bad', 'Average', 'Good']])\n",
    "    score = accuracy_score(yTest, yhatTest)\n",
    "    cost = 0\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            cost = cost + cm[i][j] * misclassificationMatrix[i][j]\n",
    "    if out:\n",
    "        print()\n",
    "        print(df)\n",
    "        print()\n",
    "        print(\"Accuracy Score: \" + str(round(score,3)) + \"%\")\n",
    "        print()\n",
    "        print(\"Misclassification cost: \" + str(cost))\n",
    "        return\n",
    "    return score, cost\n",
    "\n",
    "# Follow in order:\n",
    "# • Logistic Regression\n",
    "# • KNeighborsClassifier\n",
    "# • Support Vector Classifier (SVC)\n",
    "# • Gaussian Naive Bayes\n",
    "# • Gaussian Process Classifier\n",
    "# • Decision Tree Classifier\n",
    "# • Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\", max_iter=10e5, random_state=0)\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)\n",
    "print()\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "param = {\n",
    "    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n",
    "    'kernel':['linear', 'rbf'],\n",
    "    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n",
    "}\n",
    "grid_svc = GridSearchCV(model, param_grid=param, scoring='accuracy', cv=10)\n",
    "grid_svc.fit(xTrain, yTrain)\n",
    "bestParams = grid_svc.best_params_\n",
    "print(\"Best params: \" + str(bestParams))\n",
    "model = SVC(C = bestParams['C'], gamma =  bestParams['gamma'], kernel= bestParams['kernel'])\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianProcessClassifier()\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {\n",
    "    'n_estimators' : [100],\n",
    "    'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n",
    "    'min_samples_split' : np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "    'min_samples_leaf' : np.linspace(0.1, 0.5, 5, endpoint=True),\n",
    "    'max_features' : list(range(1, xTrain.shape[1]))\n",
    "}\n",
    "grid_rf = GridSearchCV(estimator = rf, param_grid = param, cv = 10, n_jobs = -1, verbose = 2)\n",
    "# grid_rf.fit(xTrain, yTrain) (1 Hour of tuning)\n",
    "# --> Best params: {'max_depth':1.0,'max_features':1,'min_samples_leaf':0.1,'min_samples_split':0.1,'n_estimators':100}\n",
    "#bestParams = grid_rf.best_params_\n",
    "bestParams = {\n",
    "    'max_depth': 1.0, \n",
    "    'max_features': 1, \n",
    "    'min_samples_leaf': 0.1, \n",
    "    'min_samples_split': 0.1, \n",
    "    'n_estimators': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(bestParams)\n",
    "model.fit(xTrain, yTrain)\n",
    "yhatTest = model.predict(xTest)\n",
    "evaluateClassificator(confusion_matrix(yTest, yhatTest), True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
